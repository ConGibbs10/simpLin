---
title: "Tutorial for simpLin: A fast implementation of simple linear regression"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial for simpLin: A fast implementation of simple linear regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Background on Simple Linear Regression

Simple linear regression has been used for hundreds of years, popularized in theory by [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) and named by [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton). The goal of linear regression is to find a linear function that, as accurately as possible, predicts the value of the outcome variable from the value of the predictor variable. `simpLin` leverages the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) to compute the ordinary least squares (OLS) estimates for the intercept and slope of said line. The OLS estimator is guaranteed to be the best linear unbiased estimator.

# Usage

To illustrate how to use `simpLin`, we will consider a data set which is already available in `R`. This data set is called `mtcars` and can be attained with the command

```{r}
data("mtcars")
```

This data set contains information on various cars, including the car's miles per gallon (mpg), horse power (hp), and weight (wt). For the sake of this tutorial, we are interested in modeling the miles per gallon as a linear function of the car's weight. To do this, we must first load the `simpLin` package

```{r}
library(simpLin)
```

before specifying which variable is our predictor $(x)$ and which is our response $(y)$. Note, both arguments must be numeric vectors or equal length.

```{r}
mlm <- simp_lin_R(x = mtcars$wt, y = mtcars$mpg)
```

Let's now consider the output.

# Output

After fitting the linear model, we saved the output to a variable called `mlm`. Notice, this object is a list:

```{r}
class(mlm)
```

Let's consider the each component of the output, one by one.

* `coef`: a two by one matrix of the estimated regression coefficents where the (1, 1) element is the estimate for the intercept (or $\hat{\beta}_{0}$) and the (2, 1) element is the estimate for the slope (or $\hat{\beta}_{1}$)
* `std_error`: a two by one matrix of the estimated standard errors for the coefficients where the (1, 1) element is the standard error for the intercept (or $se_{\hat{\beta_{0}}}$) and the (2, 1) element is the standard error for the slope (or $se_{\hat{\beta_{1}}}$).
* `conf_interval`: a two by two matrix representing 95% confidence intervals for the population intercept and slope. The first row is the 95% confidence interval for the intercept and the second row is the 95% confidence interval for the slope.
* `residuals`: an $n$ by 1 matrix representing the residuals associated with each observation. For a given observation $i$, the $i$th element of the matrix is given by $y_i - \hat{y}_i$.
* `predicted`: an $n$ by 1 matrix representing the predicted value for each observation. For a given observation $i$, the $i$th element of the matrix is given by $\hat{y}_i$.

# Results

We can use the results of the results of the `simp_lin_R` command to draw a scatterplot of the observed relation versus our estimated line of best fit.

```{r}
plot(mtcars$mpg~mtcars$wt, xlab = 'wt', ylab = 'mpg', pch = 20)
abline(a = mlm$coef[1,1], b = mlm$coef[2,1], lty = 'dashed')
```

Similarly, we can get the standard errors and confidence intervals for the intercept and slope

```{r, results = 'hide'}
mlm$std_error
mlm$conf_interval
```

as well as the predicted values and the residuals

```{r, results = 'hide'}
mlm$predicted
mlm$residuals
```
